# self-selective-portraiture
For recording, analysis, and generation of textual form from the Muse Headset and Eyetribe

[See documentation of the work and experiments here.](https://github.com/golanlevin/ExperimentalCapture/blob/master/students/benjamin/Project%202/project2.md)

*Using an EyeTribe Eye Tracker and Muse Headset, we simultaneously track eye gaze and brainwaves while reading a piece of text to know where an individual is looking and their corresponding level of attentiveness. Precisely aligning and combining these streams of data allows us to link textual content to level of interest and thus create a model of cognitive awareness. From this model, we use Markov chains to generate new bodies of text -- portraits of an individual -- that weight words by our level of awareness.

These portraits are expressions of text that are selected entirely by the volition of the subconscious self and, arguably, represent a more authentic expression than that which can otherwise be explicitly communicated or recalled.

Attempting what is essentially telepathy using consumer grade sensing toolkits begs the question be asked: Given the state of brain-sensing technologies, can we know what someone is thinking? How far can we peek into the brain? Will we ever be able to image thoughts and find a "blueprint" for the aesthetic experience?*

-- Collaboration with Will Miao -- Experimental Capture, Golan Levin, Carnegie Mellon University Fall 2015 --
